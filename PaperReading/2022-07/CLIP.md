# Learning Transferable Visual Models From Natural Language Supervision[CLIP]

> conference: ICML
> year: 2021

## 摘要

**模型简介**：CLIP，一个对比文本图像预训练模型，用于文本和图像。高效且易于扩展

**模型效果**：可用于多种下游任务，zero-shot正确率达到ResNet50在ImageNet上正确率

**关键词**：对比预训练、文本-图像预训练模型

## 介绍

**解决问题**：文本预训练模型对从未见过的数据，标记效果已经优于人工标记；但是在图像领域还没有达到这种效果~~（应该是为了解决图像标记的问题？）~~（用于提高图像分类或标记**zero-shot**的能力）

**主要思想**：使用自然语言的监督信号来学习图片的感知，大规模文本-图像配对

主要贡献：打破传统视觉任务需要标注的局限性；对于特征显著变化的图片仍能学习到其中的语义

## 方法

使用对比学习方法训练CLIP，使用传统的预测（逐字逐句）训练方法来训练图像是相对困难的，而使用对比学习，只需要判断文本与图像是否匹配，相对来说训练效率大大提高。

## 实验

### zero-short 迁移

**方法描述**：输入一张图片，输入n个我们感兴趣的标签，标签经过prompt engineering后得到n个句子，图像和句子经过各自的encoder之后，得到一个图片向量和n个文本向量，计算图片向量与文本向量的余弦相似度，相似度最大的文本即为模型的推理结果。

### 与`Visual N-grams`对比

数据集：ImageNet

### Prompt Engineering and Prompt Ensembling

**解决问题**：文本的多义性

**主要方法**：添加额外的语句来帮助模型确定文本的语义，还可以根据不同的数据集添加额外的语句来辅助确定。

## 局限

1. 

## 思考

- 优点
    1. 下游任务无需标注，模型的输入输出自由度比以往基于标注的模型大很多
    2. 文本与图片相绑定的训练方式，在训练的时候学习到的不仅图像的语义，还带有文本的语义，所以在zero-shot迁移学习中有更好的效果
- 缺点
    1. 效果足够强，但是还无法达到sota的效果
    2. 在一些更抽象的、更复杂的任务无法达到较好的效果
    3. 预训练需要较大的数据集
